<!DOCTYPE html>
<html>
<head>
  <title>Crop Image</title>
  <!-- <script src="https://docs.opencv.org/4.x/opencv.js" type="text/javascript"></script> -->
  
  <style>
    body { font-family: sans-serif; padding: 20px; }
    canvas { border: 1px solid #ccc; display: block; margin-top: 10px; }
    .slider-group { margin-top: 20px; }
    label { display: inline-block; width: 80px; }
    input[type="range"] { width: 300px; }
  </style>
</head>
<body>
  <h2>Auto-Cropped Image with Adjustable Size</h2>

  <div class="slider-group">
    <label for="width">Width:</label>
    <input type="range" id="width" min="200" value="300">
    <span id="widthVal">300</span>px
  </div>
  <div class="slider-group">
    <label for="height">Height:</label>
    <input type="range" id="height" min="150" value="300">
    <span id="heightVal">300</span>px
  </div>
  <h3>original image</h3>
  <img id="imgOrigin" crossorigin="anonymous" width="300px"/>
  <h3>Result</h3>
  <canvas id="canvas"></canvas>
  <div id="result" style="margin-top: 20px;">        
  </div>

  <!-- Original
  <br>
  <img id="originalImg" width="100%"/> -->

    

<script>
  
  async function setupTFJS() {
    // Try WebGL first for better performance, then fall back to CPU if needed
    try {
      await faceapi.tf.setBackend('webgl');
      console.log('TFJS using WebGL backend');
    } catch (e) {
      await faceapi.tf.setBackend('cpu');
      console.log('TFJS using CPU backend (fallback)');
    }
    await faceapi.tf.ready();
  }
  // async function setupTFJS() {
  //   await faceapi.tf.setBackend('cpu');
  //   //await faceapi.tf.setBackend('webgl'); // or 'cpu', but make sure it's supported
  //   await faceapi.tf.ready();
  //   faceapi.tf.enableProdMode();
  //   console.log('TFJS using CPU backend');
  // }
  // async function loadModels() {
  //   const modelPath = '/models'; // <-- adjust if hosted elsewhere
  //   await faceapi.nets.ssdMobilenetv1.loadFromUri(modelPath);
  //   await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath);
  //   await faceapi.nets.faceRecognitionNet.loadFromUri(modelPath);
  //   await faceapi.nets.tinyFaceDetector.loadFromUri(modelPath)
  //   // await faceapi.nets.tinyYolov2.loadFromUri(modelPath);
  //   console.log("Models loaded!");
  // }
  async function loadModels() {
    try {
      const modelPath = '/models'; // Adjust if needed
      
      // Show loading status
      console.log('Loading face detection models...');
      
      // Load models sequentially to avoid memory issues
      await faceapi.nets.ssdMobilenetv1.loadFromUri(modelPath);
      console.log('✓ SSD MobileNet loaded');
      
      await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath);
      console.log('✓ Face Landmark model loaded');
      
      await faceapi.nets.faceRecognitionNet.loadFromUri(modelPath);
      console.log('✓ Face Recognition model loaded');
      
      await faceapi.nets.tinyFaceDetector.loadFromUri(modelPath);
      console.log('✓ Tiny Face Detector loaded');
      
      console.log("All models loaded successfully!");
      return true;
    } catch (error) {
      console.error("Error loading models:", error);
      return false;
    }
  }
  let faceMatcher;
  // Helper function to ensure image is fully loaded
  function ensureImageLoaded(imgEl) {
    return new Promise((resolve) => {
      if (imgEl.complete && imgEl.naturalHeight !== 0) {
        resolve(imgEl);
      } else {
        imgEl.onload = () => resolve(imgEl);
      }
    });
  }
  // Debug helper to show image data
  function debugImageElement(img, label) {
    console.log(`${label} dimensions:`, {
      width: img.width,
      height: img.height,
      naturalWidth: img.naturalWidth,
      naturalHeight: img.naturalHeight,
      complete: img.complete
    });
  }

  async function detectAndRecognizeFace(imgEl, width, height) {
    try {
      // Ensure input image is fully loaded
      imgEl = await ensureImageLoaded(imgEl);
      debugImageElement(imgEl, "Input image");

      // Load reference image
      console.log("Loading reference image...");
      const referenceImg = await faceapi.fetchImage('/known_faces/person2.jpg');
      debugImageElement(referenceImg, "Reference image");
      
      if (referenceImg.naturalWidth === 0 || referenceImg.naturalHeight === 0) {
        console.error("Reference image failed to load properly!");
        return;
      }

      // Create a debug canvas for the reference image
      const debugCanvas = faceapi.createCanvasFromMedia(referenceImg);
      document.body.appendChild(debugCanvas);
      const debugCtx = debugCanvas.getContext('2d');
      debugCtx.drawImage(referenceImg, 0, 0);
      
      console.log("Detecting face in reference image...");
      const detectionOptions = new faceapi.SsdMobilenetv1Options({ 
        minConfidence: 0.5,
        maxResults: 1
      });
      
      const refDetections = await faceapi
        .detectSingleFace(referenceImg, detectionOptions)
        .withFaceLandmarks()
        .withFaceDescriptor();
      
      if (!refDetections) {
        console.error("No face found in reference image!");
        return;
      }

      console.log("Reference face detected:", refDetections);
      console.log("Reference descriptor:", refDetections.descriptor);

      // Visualize the detection on reference image
      const canvas = faceapi.createCanvasFromMedia(referenceImg);
      faceapi.matchDimensions(canvas, {
        width: referenceImg.naturalWidth,
        height: referenceImg.naturalHeight
      });
      document.body.appendChild(canvas);
      faceapi.draw.drawDetections(canvas, refDetections);
      faceapi.draw.drawFaceLandmarks(canvas, refDetections);

      // Create face matcher with reference face
      const labeledDescriptors = [
        new faceapi.LabeledFaceDescriptors("Person1", [refDetections.descriptor])
      ];
      faceMatcher = new faceapi.FaceMatcher(labeledDescriptors, 0.6);

      // Detect faces in the target image
      console.log("Detecting faces in target image...");
      const detections = await faceapi
        .detectAllFaces(imgEl, detectionOptions)
        .withFaceLandmarks()
        .withFaceDescriptors();

      if (!detections.length) {
        console.warn("No faces detected in target image!");
        return;
      }

      console.log(`Found ${detections.length} faces in target image:`, detections);
    
      // Process each detected face
      for (let i = 0; i < detections.length; i++) {
        const detection = detections[i];
        const dist = faceapi.euclideanDistance(detection.descriptor, refDetections.descriptor);
        console.log(`Face #${i+1} distance:`, dist);
        
        const bestMatch = faceMatcher.findBestMatch(detection.descriptor);
        console.log(`Face #${i+1} best match:`, bestMatch.toString());
        
        if (bestMatch.label !== "unknown" && bestMatch.distance < 0.6) {
          console.log(`Match found for face #${i+1}!`);
          
          // Crop the matching face
          const box = detection.detection.box;
          const cropCanvas = document.getElementById("canvas") || document.createElement("canvas");
          if (!document.getElementById("canvas")) {
            cropCanvas.id = "canvas";
            document.body.appendChild(cropCanvas);
          }
          
          const ctx = cropCanvas.getContext("2d");
          cropCanvas.width = width;
          cropCanvas.height = height;
          
          ctx.drawImage(
            imgEl,
            box.x, box.y, box.width, box.height,
            0, 0, width, height
          );
        }
      }
    }
    catch (error) {
      console.error("Error in face detection:", error);
    }

  }


  async function startDetectionCrop() {
    try {
      const w = parseInt(widthSlider.value || 300);
      const h = parseInt(heightSlider.value || 300);
      
      if (document.getElementById("widthVal")) {
        document.getElementById("widthVal").textContent = w;
      }
      if (document.getElementById("heightVal")) {
        document.getElementById("heightVal").textContent = h;
      }
      
      const img = document.getElementById("imgOrigin");
      if (!img) {
        console.error("Target image element not found!");
        return;
      }
      
      await detectAndRecognizeFace(img, w, h);
    } catch (error) {
      console.error("Error starting detection:", error);
    }
  }

  window.addEventListener("load", async () => {
    try {
      console.log("Setting up TensorFlow.js...");
      await setupTFJS();
      
      console.log("Loading face-api.js models...");
      const modelsLoaded = await loadModels();
      
      if (modelsLoaded) {
        console.log("Ready to detect faces!");
        await startDetectionCrop();
      } else {
        console.error("Failed to load models, face detection will not work.");
      }
    } catch (error) {
      console.error("Error initializing face detection:", error);
    }
  });
</script>
  <script>
    const query = new URLSearchParams(window.location.search);
    const imgPath = query.get("img");
    const fullImagePath = "/uploads/" + imgPath;
    document.getElementById("imgOrigin").src = fullImagePath;
    // const originalImg = document.getElementById("originalImg");
    // originalImg.src = fullImagePath;
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
  
    const widthSlider = document.getElementById("width");
    const heightSlider = document.getElementById("height");
    const widthVal = document.getElementById("widthVal");
    const heightVal = document.getElementById("heightVal");
  
    const cascadeFile = 'haarcascade_frontalface_default.xml';
    const cascadeUrl = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml';
  
    const img = new Image();
    img.crossOrigin = "anonymous";
    //img.src = fullImagePath;
    
    function loadCascade(filename, fileurl, onloadCallback) {
      let xhr = new XMLHttpRequest();
      xhr.open('GET', fileurl, true);
      xhr.responseType = 'arraybuffer';
      xhr.onload = function () {
        if (this.status === 200) {
          let data = new Uint8Array(this.response);
          cv.FS_createDataFile('/', filename, data, true, false, false);
          onloadCallback();
        } else {
          console.error('Failed to load cascade file');
        }
      };
      xhr.send();
    }
  
    function detectFacesAndCrop(imgEl, width, height) {
        // Resize image just for detection
        const detectWidth = Math.min(1000, imgEl.naturalWidth);
        const scaleFactor = detectWidth / imgEl.naturalWidth;

        const scaledHeight = Math.round(imgEl.naturalHeight * scaleFactor);
        const src = cv.imread(img);
        const tempCanvas = document.createElement("canvas");
        tempCanvas.width = detectWidth;
        tempCanvas.height = scaledHeight;
        const tempCtx = tempCanvas.getContext("2d");
        tempCtx.drawImage(imgEl, 0, 0, detectWidth, scaledHeight);

        const smallMat = cv.imread(tempCanvas);
        const gray = new cv.Mat();
        cv.cvtColor(smallMat, gray, cv.COLOR_RGBA2GRAY,0);

        const faces = new cv.RectVector();
        const faceCascade = new cv.CascadeClassifier();
        faceCascade.load(cascadeFile);
        var msize = new cv.Size(0, 0);
        faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0,msize,msize);
        
        const boostRegions = [];
        for (let i = 0; i < faces.size(); i++) {
            const face = faces.get(i);
            boostRegions.push({
              x: face.x / scaleFactor,
              y: face.y / scaleFactor,
              width: face.width / scaleFactor,
              height: face.height / scaleFactor,
              weight: 10.0
            });
        }
        console.log("boostRegions",boostRegions)
        SmartCrop.crop(imgEl, {
            width,
            height,
            boost: boostRegions,
            minScale: 1,
            ruleOfThirds: true,
            debug: true
        }).then(result => {
            const crop = result.topCrop;
            canvas.width = width;
            canvas.height = height;
            ctx.drawImage(
            imgEl,
            crop.x, crop.y, crop.width, crop.height,
            0, 0, width, height
            );

            let sortedResult = result.crops.sort((a,b)=>a.score.total>b.score.total ? -1 : 1);
            console.log("sortedResult",sortedResult)
            const label = document.createElement("span"); 
            label.innerHTML = "<h3>Other Results</h3>"
            document.getElementById("result").innerHTML = "";
            document.getElementById("result").appendChild(label)
            for(let i=1; i< Math.min(sortedResult.length, 8); i++){
                const crop = sortedResult[i];
                let canvas = document.createElement("canvas");
                canvas.width = width;
                canvas.height = height;
                let ctx = canvas.getContext("2d");
                ctx.drawImage(
                    imgEl,
                    crop.x, crop.y, crop.width, crop.height,
                    0, 0, width, height
                );
                document.getElementById("result").appendChild(canvas);
            }
        });

        smallMat.delete(); gray.delete(); faces.delete(); faceCascade.delete();
    }

  
    function updateCrop() {
      const w = parseInt(widthSlider.value);
      const h = parseInt(heightSlider.value);
      widthVal.textContent = w;
      heightVal.textContent = h;
      //detectFacesAndCrop(img, w, h);
    }
  
    async function openCvReady() {

      if (typeof cv === 'undefined') {
        console.error('OpenCV.js not loaded');
        return;
      }
      cv=await cv;
      console.log('OpenCV loaded');
      loadCascade(cascadeFile, cascadeUrl, function () {
        console.log('Cascade loaded');
        img.onload = () => {
            console.log("Image loaded");
          const maxW = img.naturalWidth;
          const maxH = img.naturalHeight;
  
          widthSlider.max = maxW;
          heightSlider.max = maxH;
  
          if (parseInt(widthSlider.value) > maxW) widthSlider.value = Math.floor(maxW / 2);
          if (parseInt(heightSlider.value) > maxH) heightSlider.value = Math.floor(maxH / 2);
  
          widthVal.textContent = widthSlider.value;
          heightVal.textContent = heightSlider.value;
  
          widthSlider.addEventListener("input", updateCrop);
          heightSlider.addEventListener("input", updateCrop);
  
          updateCrop(); // initial draw
        };
        img.src = fullImagePath;
      });
    
  }
  
    window.openCvReady = openCvReady;
  </script>
  
<!-- <script
  async
  src="https://unpkg.com/opencv.js@1.2.1/opencv.js"
  integrity="sha384-ucXOxPgA5tSKdaZgFD+5C0lAJeavjW31veENhNvOwsTjgx8waDD0s1QcMdUxhlxk"
  crossorigin="anonymous"
  onload="openCvReady()">
  </script> -->
 <!-- <script  async   src="https://docs.opencv.org/3.4/opencv.js"  onload="openCvReady()"></script> -->
 <script  async   src="https://docs.opencv.org/4.x/opencv.js"  onload="openCvReady()"></script>

 <script src="https://cdn.jsdelivr.net/npm/smartcrop@2.0.5/smartcrop.min.js"></script>
 <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4"></script>
 <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>


</body>
</html>